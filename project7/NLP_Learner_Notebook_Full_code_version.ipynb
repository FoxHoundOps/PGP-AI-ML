{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4gHH5-08oek3",
        "mAMOBphgo5f2",
        "vDzsF_aqpFgb",
        "67dLXJlNpl9q",
        "zyyy2l9cfZVv",
        "W8dHZozJfijz",
        "m4tEY-NdgyIr",
        "Y_NaSs5qs_Nq",
        "jzjsF0hHhPRs",
        "JkSwdOvdYNEe",
        "e7QHJ6A4vPSU",
        "PuBtH0kCRlmd",
        "Vn0UvY5zVbX6",
        "OmyiNeNMUR0j",
        "IILDJyGmWDzX",
        "LL_U6qjZYFoq",
        "55pvW09YwSsO",
        "GWAVb6_4wBTO",
        "RzTUlYq87Ydf",
        "DDOjDtje8sJS",
        "Dv_1L6Jr82Xh",
        "FxjSGUtu9KQo",
        "iyQTF6f3RX9Q",
        "yssmjSLIKTJA",
        "2-44MqJP0WcA",
        "vKG6fbJpMowr",
        "Wwz1Ts7lmx_q",
        "bpHM9_STqqSX",
        "MLW6VZof2Rr9",
        "LgEkRO5o2CH3",
        "r5lJ6sVA-tf_",
        "EaTRBxvXA0wQ",
        "50CjP1_yAto7",
        "_ehQNCnDAsyd",
        "VeDQyAxqBO47",
        "Ey0xGvtMKFMQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Computer Vision: Twitter US Airline Sentiment"
      ],
      "metadata": {
        "id": "QfbXWH36oYjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "4gHH5-08oek3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "mAMOBphgo5f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Twitter's massive user base of 330 million monthly active users presents a direct avenue for businesses to connect with a broad audience. However, the vast amount of information on the platform makes it challenging for brands to swiftly detect negative social mentions that may impact their reputation. To tackle this, sentiment analysis has become a crucial tool in social media marketing, enabling businesses to monitor emotions in conversations, understand customer sentiments, and gain insights to stay ahead in their industry.\n",
        "\n",
        "That's why sentiment analysis/classification, which involves monitoring emotions in conversations on social media platforms, has become a key strategy in social media marketing."
      ],
      "metadata": {
        "id": "kJFaBPB5o7df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "vDzsF_aqpFgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this project is to build a sentimental analysis model that classify the sentiment of tweets into the positive, neutral & negative."
      ],
      "metadata": {
        "id": "oaFfrJw7pOCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary"
      ],
      "metadata": {
        "id": "67dLXJlNpl9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* tweet_id - A unique identifier for each tweet                                                          \n",
        "* airline_sentiment - The sentiment label of the tweet, such as positive, negative, or neutral                                               \n",
        "* airline_sentiment_confidence - The confidence level associated with the sentiment label                               \n",
        "* negativereason - A category indicating the reason for negative sentiment                                                   \n",
        "* negativereason_confidence - The confidence level associated with the negative reason                                    \n",
        "*airline - The airline associated with the tweet                                                                   \n",
        "* airline_sentiment_gold - Gold standard sentiment label                                               \n",
        "* name - The username of the tweet author    \n",
        "* retweet_count - The number of times the tweet has been retweeted\n",
        "* text - The actual text content of the tweet.\n",
        "* tweet_coord - Coordinates of the tweet\n",
        "* tweet_created - The timestamp when the tweet was created\n",
        "* tweet_location - The location mentioned in the tweet\n",
        "* user_timezone - The timezone of the tweet author"
      ],
      "metadata": {
        "id": "oJl4erwwqkIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**\n",
        "\n",
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "\n",
        "* Blanks '_______' are provided in the notebook that need to be filled with an appropriate code to get the correct result\n",
        "\n",
        "* With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space\n",
        "\n",
        "* Identify the task to be performed correctly and only then proceed to write the required code\n",
        "\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\"\n",
        "\n",
        "* Running incomplete code may throw an error\n",
        "\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors\n",
        "\n",
        "* Add the results/observations derived from the analysis in the presentation and submit the same in .pdf format"
      ],
      "metadata": {
        "id": "zEG4XHjofUkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "zyyy2l9cfZVv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12uFIt9NAVES"
      },
      "source": [
        "# install and import necessary libraries.\n",
        "\n",
        "!pip install contractions\n",
        "\n",
        "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
        "import contractions                                     # Import contractions library.\n",
        "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
        "\n",
        "import numpy as np                                      # Import numpy.\n",
        "import pandas as pd                                     # Import pandas.\n",
        "import nltk                                             # Import Natural Language Tool-Kit.\n",
        "import seaborn as sns                                   # Import seaborn\n",
        "import matplotlib.pyplot as plt                         # Import Matplotlib\n",
        "\n",
        "nltk.download('stopwords')                              # Download Stopwords.\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords                                              # Import stopwords.\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize                         # Import Tokenizer.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer                                # Import Lemmatizer.\n",
        "from wordcloud import WordCloud,STOPWORDS                                      # Import WorldCloud and Stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer                    # Import count Vectorizer\n",
        "from sklearn.model_selection import train_test_split                           # Import train test split\n",
        "from sklearn.ensemble import RandomForestClassifier                            # Import Rndom Forest Classifier\n",
        "from sklearn.model_selection import cross_val_score                            # Import cross val score\n",
        "from sklearn.metrics import confusion_matrix                                   # Import confusion matrix\n",
        "from wordcloud import WordCloud                                                # Import Word Cloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                    # Import Tf-Idf vector\n",
        "import nltk                                                                    # Import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from tensorflow.keras import backend                                           # Import backend\n",
        "import random                                                                  # Import random\n",
        "import tensorflow as tf                                                        # Import tensorflow\n",
        "from sklearn.preprocessing import LabelBinarizer                               # Import Label Binarizer\n",
        "from tensorflow.keras.layers import Dropout                                    # Import Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ],
      "metadata": {
        "id": "W8dHZozJfijz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive to access the dataset  (Run this code, if you are using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gGqDXrUSfk_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039ac035-6789-4f8a-9de2-c995fa08fa92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec3j31-PCYWA"
      },
      "source": [
        "data = pd.read_csv(\"________\")               # Complete the code to read the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "m4tEY-NdgyIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- Observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- Get information about the number of rows and columns in the dataset\n",
        "- Find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected."
      ],
      "metadata": {
        "id": "tiRbnaeWtp7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the head and tail of the data"
      ],
      "metadata": {
        "id": "5Rr2etB2g3ZA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKS-Z7GiCmWX"
      },
      "source": [
        "data.__________()                            # Complete the code to display the first 5 rows of the dataset\n",
        "data.__________()                            # Complete the code to display the last 5 rows of the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand the shape of the dataset"
      ],
      "metadata": {
        "id": "Y_NaSs5qs_Nq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DysT_j1Cky-"
      },
      "source": [
        "data.___________()                           # Complete the code to get the shape of data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for Missing Values"
      ],
      "metadata": {
        "id": "jzjsF0hHhPRs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjFc0JaDCn1u"
      },
      "source": [
        "data.'_______'                               # Complete the code to check duplicate entries in the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkSwdOvdYNEe"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "e7QHJ6A4vPSU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X569zR6bIZ8"
      },
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sySnXqq2X2ro"
      },
      "source": [
        "#### Percentage of tweets for each airline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtlHwJa-baUM"
      },
      "source": [
        "labeled_barplot(data, \"_________\", perc=True)         # Complete the code to plot the labeled barplot for airline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuBtH0kCRlmd"
      },
      "source": [
        "#### Distribution of sentiments across all the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkBYhobob-6o"
      },
      "source": [
        "labeled_barplot(________, \"_____________\", perc=True) # Complete the code to plot the labeled barplot for airline_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0UvY5zVbX6"
      },
      "source": [
        "#### Plot of all the negative reasons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz1Z9HGzc-e6"
      },
      "source": [
        "labeled_barplot(________, \"______________\", perc=True)             # Complete the code to plot the labeled barplot for negative reason"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "HGHGq5v3vJ5X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmyiNeNMUR0j"
      },
      "source": [
        "#### Distribution of Sentiment of tweets for each airline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Tp_P6lThm6"
      },
      "source": [
        "airline_sentiment =  data.groupby(['________', '_________']).airline_sentiment.count().unstack()    # Complete the code to plot the barplot for the distribution of each airline with total sentiments\n",
        "airline_sentiment.plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IILDJyGmWDzX"
      },
      "source": [
        "#### Wordcloud for negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJi6zowGWjGa"
      },
      "source": [
        "airline_tweets=data[data['airline_sentiment']=='negative']\n",
        "words = ' '.join(data['text'])\n",
        "cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word\n",
        "                                and not word.startswith('@')\n",
        "                                and word != 'RT'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxoxydHWWkLl"
      },
      "source": [
        "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      background_color='black',\n",
        "                      width=3000,\n",
        "                      height=2500\n",
        "                     ).generate(cleaned_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L6KN4fdWsJI"
      },
      "source": [
        "plt.figure(1,figsize=(12, 12))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_U6qjZYFoq"
      },
      "source": [
        "#### Wordcloud for positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write the code to make the word cloud for positive tweets"
      ],
      "metadata": {
        "id": "0mKC8PAF6Cvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "55pvW09YwSsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Drop all unnecessary columns\n",
        "- Remove html tags\n",
        "- Replace contractions in string(e.g. replace I'm --> I am) and so on.\\\n",
        "- Remove numbers\n",
        "- Tokenization\n",
        "- To remove Stopwords\n",
        "- Lemmatized data"
      ],
      "metadata": {
        "id": "GSOoINzG6NEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop all unnecessary columns"
      ],
      "metadata": {
        "id": "GWAVb6_4wBTO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkHEfgc1C4aX"
      },
      "source": [
        "# Take text and airline sentiment columns from the data\n",
        "data = data[['______________', '_______________']]                      # Complete the code to get a subset of data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ONn2VkCq5N"
      },
      "source": [
        "data.______                                                             # Complete the code to display the first 5 rows of the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.______                                                             # Complete the code to get the shape of the data"
      ],
      "metadata": {
        "id": "XqwGKkmLvhIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3pLvA5aDLPp"
      },
      "source": [
        "data['___________']._________()                                         # Complete the code to display the unique values in airline sentiment column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-2sMSyaDSBU"
      },
      "source": [
        "data['___________']._________()                                         # Complete the code to display the values in airline sentiment column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove HTML Tages"
      ],
      "metadata": {
        "id": "RzTUlYq87Ydf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laSkpdl5DsgO"
      },
      "source": [
        "# Code to remove the html tage\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "data['text'] = data['___'].apply(____________)                        # Complete the code to apply strip html function on text column\n",
        "data._______                                                          # Complete the code to display the head of the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace contractions in string"
      ],
      "metadata": {
        "id": "DDOjDtje8sJS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Eedp31mEM2a"
      },
      "source": [
        "def replace_contractions(text):\n",
        "    \"\"\"Replace contractions in string of text\"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "data['_____'] = data['_________'].apply(lambda x: replace_contractions(x))                  # Complete the code to apply replace contractions function on text column\n",
        "data._______                                                                                # Complete the code to display the head of the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove numbers"
      ],
      "metadata": {
        "id": "Dv_1L6Jr82Xh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnAbvXPHEUKt"
      },
      "source": [
        "def remove_numbers(text):\n",
        "  text = re.sub(________________)                                     # Complete the code to\n",
        "  return text\n",
        "\n",
        "data['_____'] = data['_________'].apply(___________)                  # Complete the code to apply remove numbers function on text column\n",
        "data._______                                                          # Complete the code to display the head of the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Tokenization"
      ],
      "metadata": {
        "id": "FxjSGUtu9KQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)"
      ],
      "metadata": {
        "id": "rVzXg0CH8rjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_chhbPWTEX1b"
      },
      "source": [
        "# Complete the code to apply tokenization on text column\n",
        "data['_______'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "# Complete the code to display the head of the data\n",
        "data._______"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying lowercase and removing stopwords and punctuation"
      ],
      "metadata": {
        "id": "iyQTF6f3RX9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Stopwords**"
      ],
      "metadata": {
        "id": "krAVIXZJ-XZu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vkK8dzyEeA-"
      },
      "source": [
        "stopwords = stopwords.words('english')\n",
        "\n",
        "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
        "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
        "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
        "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
        "\n",
        "stopwords = list(set(stopwords) - set(customlist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All the preprocessing steps in one function**"
      ],
      "metadata": {
        "id": "bPp89PcKRbRR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpYUaYoSEhM1"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def lemmatize_list(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
        "    return new_words\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize_list(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['text'] = data.apply(lambda row: normalize(row['text']), axis=1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yssmjSLIKTJA"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using countvectorizer"
      ],
      "metadata": {
        "id": "2-44MqJP0WcA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F3Ps47KEmRu"
      },
      "source": [
        "# Vectorization (Convert text data to numbers).\n",
        "\n",
        "Count_vec = ______________(max_features=_____)                # Complete the code to initialize the CountVectorizer function with max_ features = 5000.\n",
        "data_features = Count_vec._____(data['_____'])                # Complete the code to fit and transrofm the count_vec variable on the text column\n",
        "\n",
        "data_features = data_features._______()                       # Complete the code to convert the datafram into array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_S9PgGkEqkA"
      },
      "source": [
        "data_features.___________                                     # Complete the code to check the shape of the data features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create train and test sets"
      ],
      "metadata": {
        "id": "xzvsjttyEQqW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz0x-GTgEs0o"
      },
      "source": [
        "X = _____________                                             # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                           # Complete the code to get the dependent variable (airline_sentiment) stored as Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEXKAcB4HdlJ"
      },
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=____)   # Complete the code to split the X and Y into train and test dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Model"
      ],
      "metadata": {
        "id": "XK9AEcqWqgm-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh_yexCvJSwe"
      },
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyYV-tftL3gx"
      },
      "source": [
        "#### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlIv7fWMyYPG"
      },
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkWg7NOzyZHo"
      },
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJtkIB34ydyO"
      },
      "source": [
        "# plot the error as k increases\n",
        "error = [1 - x for x in cv_scores]                                 # Error corresponds to each number of estimator\n",
        "optimal_learners = base_ln[error.index(min(error))]                # Selection of optimal number of n_estimator corresponds to minimum error.\n",
        "plt.plot(base_ln, error)                                           # Plot between each number of estimator and misclassification error\n",
        "xy = (optimal_learners, min(error))\n",
        "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
        "plt.xlabel(\"Number of base learners\")\n",
        "plt.ylabel(\"Misclassification Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHGH9AdLyijY"
      },
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Best Random Forest model"
      ],
      "metadata": {
        "id": "lyXLWEhYq_c6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBKhsHJGJVgA"
      },
      "source": [
        "  # Predict the result for test data using the model built above.\n",
        "  result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gQXCPlwJg7j"
      },
      "source": [
        "# Print and plot Confusion matirx\n",
        "\n",
        "conf_mat = ________(___________, _________)                       # Complete the code to calculate the confusion matrix between test data and result\n",
        "\n",
        "print(conf_mat)                                                   # Print confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],\n",
        "                  columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "XgjidMIXOjxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HuCfAPKMNOE"
      },
      "source": [
        "#### Wordcloud of top 40 important features from countvectorizer+Randomforest based mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1cqoVmezBmd"
      },
      "source": [
        "all_features = Count_vec.get_feature_names()                     # Instantiate the feature from the vectorizer\n",
        "top_features=''                                                  # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=','\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9sI1AqLwJaW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKG6fbJpMowr"
      },
      "source": [
        "### Using TF-IDF (Term Frequency- Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PztKcFjCJi7y"
      },
      "source": [
        "# Using TfidfVectorizer to convert text data to numbers.\n",
        "\n",
        "tfidf_vect = ______________(max_features=_____)                          # Complete the code to initialize the TF-IDF vector function with max_features = 5000.\n",
        "data_features = tfidf_vect.fit_transform(data['text'])                   # Fit the tf idf function on the text column\n",
        "\n",
        "data_features = data_features._______()                                  # Complete the code to convert the datafram into array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_features.___________                                                # Complete the code to check the shape of the data features"
      ],
      "metadata": {
        "id": "zzX31k1-WQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create train and test sets"
      ],
      "metadata": {
        "id": "Wwz1Ts7lmx_q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF3ZEo4xSk_D"
      },
      "source": [
        "X = _____________                                                        # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                                      # Complete the code to get the dependent variable (airline_sentiment) stored as Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHlq3dcS-wI"
      },
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=____)   # Complete the code to split the X and Y into train and test dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Model"
      ],
      "metadata": {
        "id": "bpHM9_STqqSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ],
      "metadata": {
        "id": "BqfuPnQ_WmBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ],
      "metadata": {
        "id": "MLW6VZof2Rr9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvfLhmUgiFdI"
      },
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ],
      "metadata": {
        "id": "ui5U4B-bWvdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neNSSIT6iO-m"
      },
      "source": [
        "# Plot the misclassification error for each of estimators (Hint: Use the above code which is used while plotting the miscalssification error for CountVector function )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ],
      "metadata": {
        "id": "qdcwI11QXS39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jiPKRA6J5-X"
      },
      "source": [
        "# Predict the result for test data using the model built above.\n",
        "result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU31UAhFJ8ip"
      },
      "source": [
        "# Plot the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, result)                      # Complete the code to calculate the confusion matrix between test data and restust\n",
        "\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(_______, annot=True, fmt='g')                         # Complete the code to plot the heatmap of the confusion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wordcloud of top 20 important features from TF-IDF+Randomforest based mode"
      ],
      "metadata": {
        "id": "LgEkRO5o2CH3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z037917nvlXQ"
      },
      "source": [
        "all_features = tfidf_vect.get_feature_names_out()          #Instantiate the feature from the vectorizer\n",
        "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=', '\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iWOlvNwlX-j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using LSTM"
      ],
      "metadata": {
        "id": "r5lJ6sVA-tf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing backend\n",
        "backend.clear_session()\n",
        "# Fixing the seed for random number generators\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "oFQw4e2g-v9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing the text column"
      ],
      "metadata": {
        "id": "EaTRBxvXA0wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code with by having 800 vocab size\n",
        "tokenizer = Tokenizer(num_words = ____, split = ' ')\n",
        "\n",
        "# Complete the code to fit tokenizer on text data\n",
        "tokenizer.fit_on_texts(data['____'].values)\n",
        "\n",
        "# Converting text to sequences\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "# Padding the sequences\n",
        "X = pad_sequences(X)"
      ],
      "metadata": {
        "id": "fWSTAjHF-v6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding the target variable"
      ],
      "metadata": {
        "id": "50CjP1_yAto7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the Label Binarizer\n",
        "enc = LabelBinarizer()\n",
        "# Fitting the Label Binarizer on airline_sentiment\n",
        "y_encoded = enc.fit_transform(data['airline_sentiment'])"
      ],
      "metadata": {
        "id": "kRxtDavL-v48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the data into train and test"
      ],
      "metadata": {
        "id": "_ehQNCnDAsyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.30, random_state = 42)"
      ],
      "metadata": {
        "id": "NJywu9ge-v3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training LSTM Model"
      ],
      "metadata": {
        "id": "VeDQyAxqBO47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding the embedding layer with 800 vocabularies, 120 neurons\n",
        "model.add(Embedding(___, ____, input_length = X.shape[1]))\n",
        "\n",
        "# Complete the code to add the LSTM layer with 256 neurons\n",
        "model.add(LSTM(___,return_sequences=True))\n",
        "\n",
        "# Complete the code to add the LSTM layer with 150 neurons and dropout_rate= 0.2\n",
        "model.add(LSTM(___, dropout = ___, recurrent_dropout = 0.2))\n",
        "\n",
        "# Complete the code to add the dense layer with 124 neurons and relu activation function\n",
        "model.add(Dense(___,activation = '___'))\n",
        "\n",
        "# Complete the code to add dropout with dropout_rate= 0.2\n",
        "model.add(Dropout(____))\n",
        "\n",
        "# Complete the code to add a dense layer with 64 neurons and relu activation function\n",
        "model.add(Dense(___,activation = '___'))\n",
        "\n",
        "# Complete to the code to add the output layer with 3 neurons and softmax activation function\n",
        "model.add(Dense(___, activation = '___'))\n",
        "\n",
        "# Complete the code to compile the model with categorical_crossentropy as loss function, accuracy as metrics and adam as optimizer\n",
        "model.compile(loss = '______', optimizer = '_____', metrics = ['_____'])"
      ],
      "metadata": {
        "id": "ccs6sQr4-v1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of the model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "tL1Ytmxa-vy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Complete the code to fit the model on X_train and y_train with epochs as 30, batch size as 32\n",
        "his = model.fit(X_train, y_train, epochs = ___, batch_size = ___, verbose = 'auto')"
      ],
      "metadata": {
        "id": "BROYiR2z-vwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on X_test using the above model\n",
        "result = model.predict(X_test)"
      ],
      "metadata": {
        "id": "UBVyiAge-vt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying argmax function on the predicted values (result) to get the predicted labels\n",
        "y_pred_arg=np.argmax(result,axis=1)\n",
        "# Applying argmax function on the y_test to get back the predicted labels\n",
        "y_test_arg=np.argmax(y_test,axis=1)"
      ],
      "metadata": {
        "id": "lzJHYk7u-vrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the confusion matrix**"
      ],
      "metadata": {
        "id": "Ax2QUGmNH7WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat = confusion_matrix(y_test_arg, y_pred_arg)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],\n",
        "                  columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')"
      ],
      "metadata": {
        "id": "oUrPuf8a-voe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey0xGvtMKFMQ"
      },
      "source": [
        "## Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\n"
      ],
      "metadata": {
        "id": "S-xokNC0pq-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Happy Learning!\n",
        "---"
      ],
      "metadata": {
        "id": "Z0lxKGC-nL3_"
      }
    }
  ]
}