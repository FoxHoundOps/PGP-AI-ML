{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Learner Name: Damian Najera**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfbXWH36oYjf"
      },
      "source": [
        "# Introduction to Computer Vision: Twitter US Airline Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gHH5-08oek3"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAMOBphgo5f2"
      },
      "source": [
        "### Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJFaBPB5o7df"
      },
      "source": [
        "Twitter's massive user base of 330 million monthly active users presents a direct avenue for businesses to connect with a broad audience. However, the vast amount of information on the platform makes it challenging for brands to swiftly detect negative social mentions that may impact their reputation. To tackle this, sentiment analysis has become a crucial tool in social media marketing, enabling businesses to monitor emotions in conversations, understand customer sentiments, and gain insights to stay ahead in their industry.\n",
        "\n",
        "That's why sentiment analysis/classification, which involves monitoring emotions in conversations on social media platforms, has become a key strategy in social media marketing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDzsF_aqpFgb"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaFfrJw7pOCd"
      },
      "source": [
        "The aim of this project is to build a sentimental analysis model that classify the sentiment of tweets into the positive, neutral & negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67dLXJlNpl9q"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJl4erwwqkIV"
      },
      "source": [
        "* tweet_id - A unique identifier for each tweet                                                          \n",
        "* airline_sentiment - The sentiment label of the tweet, such as positive, negative, or neutral                                               \n",
        "* airline_sentiment_confidence - The confidence level associated with the sentiment label                               \n",
        "* negativereason - A category indicating the reason for negative sentiment                                                   \n",
        "* negativereason_confidence - The confidence level associated with the negative reason                                    \n",
        "*airline - The airline associated with the tweet                                                                   \n",
        "* airline_sentiment_gold - Gold standard sentiment label                                               \n",
        "* name - The username of the tweet author    \n",
        "* retweet_count - The number of times the tweet has been retweeted\n",
        "* text - The actual text content of the tweet.\n",
        "* tweet_coord - Coordinates of the tweet\n",
        "* tweet_created - The timestamp when the tweet was created\n",
        "* tweet_location - The location mentioned in the tweet\n",
        "* user_timezone - The timezone of the tweet author"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyy2l9cfZVv"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "12uFIt9NAVES"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\BD\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\BD\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\BD\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\BD\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
        "import contractions                                     # Import contractions library.\n",
        "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
        "\n",
        "import numpy as np                                      # Import numpy.\n",
        "import pandas as pd                                     # Import pandas.\n",
        "import nltk                                             # Import Natural Language Tool-Kit.\n",
        "import seaborn as sns                                   # Import seaborn\n",
        "import matplotlib.pyplot as plt                         # Import Matplotlib\n",
        "\n",
        "nltk.download('stopwords')                              # Download Stopwords.\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords                                              # Import stopwords.\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize                         # Import Tokenizer.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer                                # Import Lemmatizer.\n",
        "from wordcloud import WordCloud,STOPWORDS                                      # Import WorldCloud and Stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer                    # Import count Vectorizer\n",
        "from sklearn.model_selection import train_test_split                           # Import train test split\n",
        "from sklearn.ensemble import RandomForestClassifier                            # Import Rndom Forest Classifier\n",
        "from sklearn.model_selection import cross_val_score                            # Import cross val score\n",
        "from sklearn.metrics import confusion_matrix                                   # Import confusion matrix\n",
        "from wordcloud import WordCloud                                                # Import Word Cloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                    # Import Tf-Idf vector\n",
        "import nltk                                                                    # Import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from tensorflow.keras import backend                                           # Import backend\n",
        "import random                                                                  # Import random\n",
        "import tensorflow as tf                                                        # Import tensorflow\n",
        "from sklearn.preprocessing import LabelBinarizer                               # Import Label Binarizer\n",
        "from tensorflow.keras.layers import Dropout                                    # Import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8dHZozJfijz"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ec3j31-PCYWA"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"Tweets.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4tEY-NdgyIr"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRbnaeWtp7Z"
      },
      "source": [
        "The initial steps to get an overview of any dataset is to:\n",
        "- Observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- Get information about the number of rows and columns in the dataset\n",
        "- Find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rr2etB2g3ZA"
      },
      "source": [
        "### Check the head and tail of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
              "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
              "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MKS-Z7GiCmWX"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>569587686496825344</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3487</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>KristenReenders</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir thank you we got on a different f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 12:01:01 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>569587371693355008</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>itsropes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:46 -0800</td>\n",
              "      <td>Texas</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14637</th>\n",
              "      <td>569587242672398336</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sanyabun</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:15 -0800</td>\n",
              "      <td>Nigeria,lagos</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14638</th>\n",
              "      <td>569587188687634433</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>0.6659</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SraJackson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir you have my money, you change my ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:02 -0800</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14639</th>\n",
              "      <td>569587140490866689</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6771</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>daviddtwu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:58:51 -0800</td>\n",
              "      <td>dallas, TX</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "14635  569587686496825344          positive                        0.3487   \n",
              "14636  569587371693355008          negative                        1.0000   \n",
              "14637  569587242672398336           neutral                        1.0000   \n",
              "14638  569587188687634433          negative                        1.0000   \n",
              "14639  569587140490866689           neutral                        0.6771   \n",
              "\n",
              "               negativereason  negativereason_confidence   airline  \\\n",
              "14635                     NaN                     0.0000  American   \n",
              "14636  Customer Service Issue                     1.0000  American   \n",
              "14637                     NaN                        NaN  American   \n",
              "14638  Customer Service Issue                     0.6659  American   \n",
              "14639                     NaN                     0.0000  American   \n",
              "\n",
              "      airline_sentiment_gold             name negativereason_gold  \\\n",
              "14635                    NaN  KristenReenders                 NaN   \n",
              "14636                    NaN         itsropes                 NaN   \n",
              "14637                    NaN         sanyabun                 NaN   \n",
              "14638                    NaN       SraJackson                 NaN   \n",
              "14639                    NaN        daviddtwu                 NaN   \n",
              "\n",
              "       retweet_count                                               text  \\\n",
              "14635              0  @AmericanAir thank you we got on a different f...   \n",
              "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
              "14637              0  @AmericanAir Please bring American Airlines to...   \n",
              "14638              0  @AmericanAir you have my money, you change my ...   \n",
              "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
              "\n",
              "      tweet_coord              tweet_created tweet_location  \\\n",
              "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
              "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
              "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
              "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
              "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
              "\n",
              "                    user_timezone  \n",
              "14635                         NaN  \n",
              "14636                         NaN  \n",
              "14637                         NaN  \n",
              "14638  Eastern Time (US & Canada)  \n",
              "14639                         NaN  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_NaSs5qs_Nq"
      },
      "source": [
        "### Understand the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7DysT_j1Cky-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for Duplicate Entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 36\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_rows = data.duplicated().sum()\n",
        "\n",
        "# Display the number of duplicate rows\n",
        "print(f\"Number of duplicate rows: {duplicate_rows}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the context of our problem and the understanding of our dataset, let us remove the duplicate entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing duplicate rows\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Verifying that duplicates have been removed\n",
        "remaining_duplicates = data.duplicated().sum()\n",
        "\n",
        "remaining_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzjsF0hHhPRs"
      },
      "source": [
        "### Checking for Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RjFc0JaDCn1u"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tweet_id                            0\n",
              "airline_sentiment                   0\n",
              "airline_sentiment_confidence        0\n",
              "negativereason                   5445\n",
              "negativereason_confidence        4101\n",
              "airline                             0\n",
              "airline_sentiment_gold          14564\n",
              "name                                0\n",
              "negativereason_gold             14572\n",
              "retweet_count                       0\n",
              "text                                0\n",
              "tweet_coord                     13589\n",
              "tweet_created                       0\n",
              "tweet_location                   4723\n",
              "user_timezone                    4814\n",
              "dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for total missing values in each column\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Display the columns with their respective count of missing values\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observations:\n",
        "- `negativereason`: 5,445 missing values. This is expected since not every tweet will be negative, and thus won't have a reason associated with it.\n",
        "- `negativereason_confidence`: 4,101 missing values. Similar to the previous point, not every tweet will have a negative reason confidence if it's not negative.\n",
        "- `airline_sentiment_gold`: 14,564 missing values. This column seems to represent some gold standard for sentiment, but it's mostly missing. Given its high count of missing values, it might not be very useful for analysis or modeling.\n",
        "negativereason_gold: 14,572 missing values. Like airline_sentiment_gold, this column has a high number of missing values, which makes it less useful.\n",
        "- `negativereason_gold`: 14,572 missing values. Like airline_sentiment_gold, this column has a high number of missing values, which makes it less useful.\n",
        "- `tweet_coord`: 13,589 missing values. This indicates that a large portion of tweets do not have geolocation data associated with them.\n",
        "- `tweet_location`: 4,723 missing values. This suggests that many users have not specified a location in their tweets or profiles.\n",
        "- `user_timezone`: 4,814 missing values. A significant number of users haven't set or provided their timezones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkSwdOvdYNEe"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7QHJ6A4vPSU"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0X569zR6bIZ8"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sySnXqq2X2ro"
      },
      "source": [
        "#### Percentage of tweets for each airline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtlHwJa-baUM"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_________\", perc=True)         # Complete the code to plot the labeled barplot for airline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuBtH0kCRlmd"
      },
      "source": [
        "#### Distribution of sentiments across all the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkBYhobob-6o"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(________, \"_____________\", perc=True) # Complete the code to plot the labeled barplot for airline_sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn0UvY5zVbX6"
      },
      "source": [
        "#### Plot of all the negative reasons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz1Z9HGzc-e6"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(________, \"______________\", perc=True)             # Complete the code to plot the labeled barplot for negative reason"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGHGq5v3vJ5X"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmyiNeNMUR0j"
      },
      "source": [
        "#### Distribution of Sentiment of tweets for each airline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Tp_P6lThm6"
      },
      "outputs": [],
      "source": [
        "airline_sentiment =  data.groupby(['________', '_________']).airline_sentiment.count().unstack()    # Complete the code to plot the barplot for the distribution of each airline with total sentiments\n",
        "airline_sentiment.plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IILDJyGmWDzX"
      },
      "source": [
        "#### Wordcloud for negative tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJi6zowGWjGa"
      },
      "outputs": [],
      "source": [
        "airline_tweets=data[data['airline_sentiment']=='negative']\n",
        "words = ' '.join(data['text'])\n",
        "cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word\n",
        "                                and not word.startswith('@')\n",
        "                                and word != 'RT'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxoxydHWWkLl"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(stopwords=STOPWORDS,\n",
        "                      background_color='black',\n",
        "                      width=3000,\n",
        "                      height=2500\n",
        "                     ).generate(cleaned_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L6KN4fdWsJI"
      },
      "outputs": [],
      "source": [
        "plt.figure(1,figsize=(12, 12))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_U6qjZYFoq"
      },
      "source": [
        "#### Wordcloud for positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mKC8PAF6Cvr"
      },
      "outputs": [],
      "source": [
        "# write the code to make the word cloud for positive tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55pvW09YwSsO"
      },
      "source": [
        "## Data Preparation for Modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSOoINzG6NEN"
      },
      "source": [
        "- Drop all unnecessary columns\n",
        "- Remove html tags\n",
        "- Replace contractions in string(e.g. replace I'm --> I am) and so on.\\\n",
        "- Remove numbers\n",
        "- Tokenization\n",
        "- To remove Stopwords\n",
        "- Lemmatized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWAVb6_4wBTO"
      },
      "source": [
        "### Drop all unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkHEfgc1C4aX"
      },
      "outputs": [],
      "source": [
        "# Take text and airline sentiment columns from the data\n",
        "data = data[['______________', '_______________']]                      # Complete the code to get a subset of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8ONn2VkCq5N"
      },
      "outputs": [],
      "source": [
        "data.______                                                             # Complete the code to display the first 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqwGKkmLvhIM"
      },
      "outputs": [],
      "source": [
        "data.______                                                             # Complete the code to get the shape of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3pLvA5aDLPp"
      },
      "outputs": [],
      "source": [
        "data['___________']._________()                                         # Complete the code to display the unique values in airline sentiment column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-2sMSyaDSBU"
      },
      "outputs": [],
      "source": [
        "data['___________']._________()                                         # Complete the code to display the values in airline sentiment column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTUlYq87Ydf"
      },
      "source": [
        "### Remove HTML Tages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laSkpdl5DsgO"
      },
      "outputs": [],
      "source": [
        "# Code to remove the html tage\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "data['text'] = data['___'].apply(____________)                        # Complete the code to apply strip html function on text column\n",
        "data._______                                                          # Complete the code to display the head of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDOjDtje8sJS"
      },
      "source": [
        "### Replace contractions in string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Eedp31mEM2a"
      },
      "outputs": [],
      "source": [
        "def replace_contractions(text):\n",
        "    \"\"\"Replace contractions in string of text\"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "data['_____'] = data['_________'].apply(lambda x: replace_contractions(x))                  # Complete the code to apply replace contractions function on text column\n",
        "data._______                                                                                # Complete the code to display the head of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv_1L6Jr82Xh"
      },
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnAbvXPHEUKt"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(text):\n",
        "  text = re.sub(________________)                                     # Complete the code to\n",
        "  return text\n",
        "\n",
        "data['_____'] = data['_________'].apply(___________)                  # Complete the code to apply remove numbers function on text column\n",
        "data._______                                                          # Complete the code to display the head of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxjSGUtu9KQo"
      },
      "source": [
        "### Apply Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVzXg0CH8rjC"
      },
      "outputs": [],
      "source": [
        "data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_chhbPWTEX1b"
      },
      "outputs": [],
      "source": [
        "# Complete the code to apply tokenization on text column\n",
        "data['_______'] = data.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "# Complete the code to display the head of the data\n",
        "data._______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyQTF6f3RX9Q"
      },
      "source": [
        "### Applying lowercase and removing stopwords and punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krAVIXZJ-XZu"
      },
      "source": [
        "**Adding Stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vkK8dzyEeA-"
      },
      "outputs": [],
      "source": [
        "stopwords = stopwords.words('english')\n",
        "\n",
        "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
        "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
        "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
        "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
        "\n",
        "stopwords = list(set(stopwords) - set(customlist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPp89PcKRbRR"
      },
      "source": [
        "**All the preprocessing steps in one function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpYUaYoSEhM1"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def lemmatize_list(words):\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
        "    return new_words\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize_list(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['text'] = data.apply(lambda row: normalize(row['text']), axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yssmjSLIKTJA"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-44MqJP0WcA"
      },
      "source": [
        "### Using countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F3Ps47KEmRu"
      },
      "outputs": [],
      "source": [
        "# Vectorization (Convert text data to numbers).\n",
        "\n",
        "Count_vec = ______________(max_features=_____)                # Complete the code to initialize the CountVectorizer function with max_ features = 5000.\n",
        "data_features = Count_vec._____(data['_____'])                # Complete the code to fit and transrofm the count_vec variable on the text column\n",
        "\n",
        "data_features = data_features._______()                       # Complete the code to convert the datafram into array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_S9PgGkEqkA"
      },
      "outputs": [],
      "source": [
        "data_features.___________                                     # Complete the code to check the shape of the data features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzvsjttyEQqW"
      },
      "source": [
        "#### Create train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz0x-GTgEs0o"
      },
      "outputs": [],
      "source": [
        "X = _____________                                             # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                           # Complete the code to get the dependent variable (airline_sentiment) stored as Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEXKAcB4HdlJ"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=____)   # Complete the code to split the X and Y into train and test dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK9AEcqWqgm-"
      },
      "source": [
        "#### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh_yexCvJSwe"
      },
      "outputs": [],
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyYV-tftL3gx"
      },
      "source": [
        "#### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlIv7fWMyYPG"
      },
      "outputs": [],
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkWg7NOzyZHo"
      },
      "outputs": [],
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJtkIB34ydyO"
      },
      "outputs": [],
      "source": [
        "# plot the error as k increases\n",
        "error = [1 - x for x in cv_scores]                                 # Error corresponds to each number of estimator\n",
        "optimal_learners = base_ln[error.index(min(error))]                # Selection of optimal number of n_estimator corresponds to minimum error.\n",
        "plt.plot(base_ln, error)                                           # Plot between each number of estimator and misclassification error\n",
        "xy = (optimal_learners, min(error))\n",
        "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
        "plt.xlabel(\"Number of base learners\")\n",
        "plt.ylabel(\"Misclassification Error\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHGH9AdLyijY"
      },
      "outputs": [],
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyXLWEhYq_c6"
      },
      "source": [
        "#### Best Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBKhsHJGJVgA"
      },
      "outputs": [],
      "source": [
        "  # Predict the result for test data using the model built above.\n",
        "  result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gQXCPlwJg7j"
      },
      "outputs": [],
      "source": [
        "# Print and plot Confusion matirx\n",
        "\n",
        "conf_mat = ________(___________, _________)                       # Complete the code to calculate the confusion matrix between test data and result\n",
        "\n",
        "print(conf_mat)                                                   # Print confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgjidMIXOjxv"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],\n",
        "                  columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HuCfAPKMNOE"
      },
      "source": [
        "#### Wordcloud of top 40 important features from countvectorizer+Randomforest based mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1cqoVmezBmd"
      },
      "outputs": [],
      "source": [
        "all_features = Count_vec.get_feature_names()                     # Instantiate the feature from the vectorizer\n",
        "top_features=''                                                  # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=','\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sI1AqLwJaW0"
      },
      "outputs": [],
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKG6fbJpMowr"
      },
      "source": [
        "### Using TF-IDF (Term Frequency- Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztKcFjCJi7y"
      },
      "outputs": [],
      "source": [
        "# Using TfidfVectorizer to convert text data to numbers.\n",
        "\n",
        "tfidf_vect = ______________(max_features=_____)                          # Complete the code to initialize the TF-IDF vector function with max_features = 5000.\n",
        "data_features = tfidf_vect.fit_transform(data['text'])                   # Fit the tf idf function on the text column\n",
        "\n",
        "data_features = data_features._______()                                  # Complete the code to convert the datafram into array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzX31k1-WQxC"
      },
      "outputs": [],
      "source": [
        "data_features.___________                                                # Complete the code to check the shape of the data features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwz1Ts7lmx_q"
      },
      "source": [
        "#### Create train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF3ZEo4xSk_D"
      },
      "outputs": [],
      "source": [
        "X = _____________                                                        # Complete the code to get the independent variable (data_features) stored as X\n",
        "\n",
        "y = data.__________                                                      # Complete the code to get the dependent variable (airline_sentiment) stored as Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNHlq3dcS-wI"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing set.\n",
        "\n",
        "X_train, X_test, y_train, y_test =_________ (__, __, test_size=___, random_state=____)   # Complete the code to split the X and Y into train and test dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpHM9_STqqSX"
      },
      "source": [
        "#### Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqfuPnQ_WmBD"
      },
      "outputs": [],
      "source": [
        "# Using Random Forest to build model for the classification of reviews.\n",
        "\n",
        "forest = ____________(n_estimators=____, n_jobs=4)            # Initialize the Random Forest Classifier\n",
        "\n",
        "forest = ______.____(______, _______)                         # Fit the forest variable on X_train and y_train\n",
        "\n",
        "print(forest)\n",
        "\n",
        "print(np.mean(_______________(forest, X, y, cv=10)))          # Calculate cross validation score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLW6VZof2Rr9"
      },
      "source": [
        "#### Optimize the parameter: The number of trees in the random forest model(n_estimators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvfLhmUgiFdI"
      },
      "outputs": [],
      "source": [
        "# Finding optimal number of base learners using k-fold CV ->\n",
        "base_ln = [x for x in range(1, 25)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui5U4B-bWvdd"
      },
      "outputs": [],
      "source": [
        "# K-Fold Cross - validation .\n",
        "cv_scores = []                                                                             # Initializing a emptry list to store the score\n",
        "for b in base_ln:\n",
        "    clf = _______________(n_estimators = b)                                                # Complete the code to apply Rondome Forest Classifier\n",
        "    scores = ___________(_____, ______, _______, cv = 5, scoring = '___________')          # Complete the code to find the cross-validation score on the classifier (clf) for accuracy\n",
        "    cv_scores.append(scores.mean())                                                        # Append the scores to cv_scores list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neNSSIT6iO-m"
      },
      "outputs": [],
      "source": [
        "# Plot the misclassification error for each of estimators (Hint: Use the above code which is used while plotting the miscalssification error for CountVector function )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdcwI11QXS39"
      },
      "outputs": [],
      "source": [
        "# Train the best model and calculating accuracy on test data .\n",
        "clf = _________(n_estimators = _____________)                     # Initialize the Random Forest classifier with optimal learners\n",
        "___.____(____, ___)                                               # Fit the classifer on X_train and y_train\n",
        "___.____(____, ___)                                               # Find the score on X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jiPKRA6J5-X"
      },
      "outputs": [],
      "source": [
        "# Predict the result for test data using the model built above.\n",
        "result = _____.predict(_______)                                   # Complete the code to predict the X_test data using the model built above (forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU31UAhFJ8ip"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, result)                      # Complete the code to calculate the confusion matrix between test data and restust\n",
        "\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(_______, annot=True, fmt='g')                         # Complete the code to plot the heatmap of the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgEkRO5o2CH3"
      },
      "source": [
        "#### Wordcloud of top 20 important features from TF-IDF+Randomforest based mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z037917nvlXQ"
      },
      "outputs": [],
      "source": [
        "all_features = tfidf_vect.get_feature_names_out()          #Instantiate the feature from the vectorizer\n",
        "top_features=''                                            # Addition of top 40 feature into top_feature after training the model\n",
        "feat=clf.feature_importances_\n",
        "features=np.argsort(feat)[::-1]\n",
        "for i in features[0:40]:\n",
        "    top_features+=all_features[i]\n",
        "    top_features+=', '\n",
        "\n",
        "print(top_features)\n",
        "\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "\n",
        "# Complete the code by applying wordcloud on top features\n",
        "wordcloud = ________(background_color=\"white\",colormap='viridis',width=2000,height=1000).generate(_______)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWOlvNwlX-j9"
      },
      "outputs": [],
      "source": [
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figure(1, figsize=(14, 11), frameon='equal')\n",
        "plt.title('Top 40 features WordCloud', fontsize=20)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5lJ6sVA-tf_"
      },
      "source": [
        "### Using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFQw4e2g-v9v"
      },
      "outputs": [],
      "source": [
        "# Clearing backend\n",
        "backend.clear_session()\n",
        "# Fixing the seed for random number generators\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaTRBxvXA0wQ"
      },
      "source": [
        "#### Tokenizing the text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWSTAjHF-v6y"
      },
      "outputs": [],
      "source": [
        "# Complete the code with by having 800 vocab size\n",
        "tokenizer = Tokenizer(num_words = ____, split = ' ')\n",
        "\n",
        "# Complete the code to fit tokenizer on text data\n",
        "tokenizer.fit_on_texts(data['____'].values)\n",
        "\n",
        "# Converting text to sequences\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "# Padding the sequences\n",
        "X = pad_sequences(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50CjP1_yAto7"
      },
      "source": [
        "#### Encoding the target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRxtDavL-v48"
      },
      "outputs": [],
      "source": [
        "# Storing the Label Binarizer\n",
        "enc = LabelBinarizer()\n",
        "# Fitting the Label Binarizer on airline_sentiment\n",
        "y_encoded = enc.fit_transform(data['airline_sentiment'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ehQNCnDAsyd"
      },
      "source": [
        "#### Split the data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJywu9ge-v3H"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size = 0.30, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDQyAxqBO47"
      },
      "source": [
        "#### Training LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccs6sQr4-v1f"
      },
      "outputs": [],
      "source": [
        "# Initializing the model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding the embedding layer with 800 vocabularies, 120 neurons\n",
        "model.add(Embedding(___, ____, input_length = X.shape[1]))\n",
        "\n",
        "# Complete the code to add the LSTM layer with 256 neurons\n",
        "model.add(LSTM(___,return_sequences=True))\n",
        "\n",
        "# Complete the code to add the LSTM layer with 150 neurons and dropout_rate= 0.2\n",
        "model.add(LSTM(___, dropout = ___, recurrent_dropout = 0.2))\n",
        "\n",
        "# Complete the code to add the dense layer with 124 neurons and relu activation function\n",
        "model.add(Dense(___,activation = '___'))\n",
        "\n",
        "# Complete the code to add dropout with dropout_rate= 0.2\n",
        "model.add(Dropout(____))\n",
        "\n",
        "# Complete the code to add a dense layer with 64 neurons and relu activation function\n",
        "model.add(Dense(___,activation = '___'))\n",
        "\n",
        "# Complete to the code to add the output layer with 3 neurons and softmax activation function\n",
        "model.add(Dense(___, activation = '___'))\n",
        "\n",
        "# Complete the code to compile the model with categorical_crossentropy as loss function, accuracy as metrics and adam as optimizer\n",
        "model.compile(loss = '______', optimizer = '_____', metrics = ['_____'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL1Ytmxa-vy0"
      },
      "outputs": [],
      "source": [
        "# Summary of the model\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BROYiR2z-vwo"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Complete the code to fit the model on X_train and y_train with epochs as 30, batch size as 32\n",
        "his = model.fit(X_train, y_train, epochs = ___, batch_size = ___, verbose = 'auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBVyiAge-vt7"
      },
      "outputs": [],
      "source": [
        "# Predicting on X_test using the above model\n",
        "result = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzJHYk7u-vrN"
      },
      "outputs": [],
      "source": [
        "# Applying argmax function on the predicted values (result) to get the predicted labels\n",
        "y_pred_arg=np.argmax(result,axis=1)\n",
        "# Applying argmax function on the y_test to get back the predicted labels\n",
        "y_test_arg=np.argmax(y_test,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax2QUGmNH7WY"
      },
      "source": [
        "**Plotting the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUrPuf8a-voe"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(y_test_arg, y_pred_arg)\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['positive', 'negative', 'neutral']],\n",
        "                  columns = [i for i in ['positive', 'negative', 'neutral']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey0xGvtMKFMQ"
      },
      "source": [
        "## Summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-xokNC0pq-R"
      },
      "source": [
        "-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0lxKGC-nL3_"
      },
      "source": [
        "## Happy Learning!\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4gHH5-08oek3",
        "mAMOBphgo5f2",
        "vDzsF_aqpFgb",
        "67dLXJlNpl9q",
        "zyyy2l9cfZVv",
        "W8dHZozJfijz",
        "m4tEY-NdgyIr",
        "Y_NaSs5qs_Nq",
        "jzjsF0hHhPRs",
        "JkSwdOvdYNEe",
        "e7QHJ6A4vPSU",
        "PuBtH0kCRlmd",
        "Vn0UvY5zVbX6",
        "OmyiNeNMUR0j",
        "IILDJyGmWDzX",
        "LL_U6qjZYFoq",
        "55pvW09YwSsO",
        "GWAVb6_4wBTO",
        "RzTUlYq87Ydf",
        "DDOjDtje8sJS",
        "Dv_1L6Jr82Xh",
        "FxjSGUtu9KQo",
        "iyQTF6f3RX9Q",
        "yssmjSLIKTJA",
        "2-44MqJP0WcA",
        "vKG6fbJpMowr",
        "Wwz1Ts7lmx_q",
        "bpHM9_STqqSX",
        "MLW6VZof2Rr9",
        "LgEkRO5o2CH3",
        "r5lJ6sVA-tf_",
        "EaTRBxvXA0wQ",
        "50CjP1_yAto7",
        "_ehQNCnDAsyd",
        "VeDQyAxqBO47",
        "Ey0xGvtMKFMQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
